{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA) [50 pts]\n",
    "In this part of the exercise, you will re-visit the problem of predicting whether a student gets admitted into a university. However, in this part, you will build a linear discriminant analysis (LDA) classifier for this problem.\n",
    "\n",
    "LDA is a generative model for classification that assumes the class covariances are equal. Given a training dataset of positive and negative features (x, y) with y $\\in$ {0, 1} , LDA models the data x as generated from class-conditional Gaussians:\n",
    "\n",
    "$P(x, y) = P(x|y)P(y)$ where $P(y = 1) = \\pi$ and $P(x|y) = N(x;\\mu^y,\\Sigma)$\n",
    "\n",
    "where means $\\mu^y$ are class-dependent but the covariance matrix $\\Sigma$ is class-independent (the same for all classes).\n",
    "\n",
    "A novel feature $x$ is classified as a positive if $P(y = 1|x) > P(y = 0|x)$, whichis equivalent to $a(x)\\gt0$, where the linear classifier $a(x) = w^Tx+w_0$ has weights given by $w = \\Sigma^{-1}(\\mu^1-\\mu^0)$.\n",
    "\n",
    "In practice, and in this assignment, we use $a(x)\\gt$ some threshold, or equivalently, $w^Tx>T$ for some constant $T$.\n",
    "\n",
    "As we saw in lecture, LDA and logistic regression can be expressed in the same form\n",
    "\n",
    "$P(y=1|x) = \\frac{1}{1+e^{-\\theta^Tx}}.$\n",
    "\n",
    "However, they generally produce different solutions for the parameter theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "In this assignment, you can assume the prior probabilities for the two classes are the same (although the number of the positive and negative samples in the training data is not the same), and that the threshold $T$ is zero. As a bonus, you are encouraged to explore how the different prior probabilities shift the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAF3CAYAAADgjOwXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe488312748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "datafile = 'data/ex2data1.txt'\n",
    "#!head $datafile\n",
    "cols = np.loadtxt(datafile, delimiter=',', usecols=(0, 1, 2), unpack=True)  #Read in comma separated data\n",
    "##Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size  # number of training examples\n",
    "##Insert the usual column of 1's into the \"X\" matrix\n",
    "X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "#Divide the sample into two: ones with positive classification, one with null classification\n",
    "pos = np.array([X[i] for i in range(X.shape[0]) if y[i] == 1])\n",
    "neg = np.array([X[i] for i in range(X.shape[0]) if y[i] == 0])\n",
    "\n",
    "def plotData():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(pos[:, 1], pos[:, 2], 'k+', label='Admitted')\n",
    "    plt.plot(neg[:, 1], neg[:, 2], 'yo', label='Not admitted')\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plotData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the LDA classifier by completing the code here. As an implementation detail, you should first center the positive and negative data separately, so that each has a mean equal to 0, before computing the covariance, as this tends to give a more accurate estimate. \n",
    "\n",
    "You should center the whole training data set before applying the classifier. Namely, subtract the middle value of the two classes’ means (12(pos mean+neg mean)), which is on the separating plane when their prior probabilities are the same and becomes the ‘center’ of the data. [10pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_mean = np.mean(pos[:,1:], axis=0)\n",
    "neg_mean = np.mean(neg[:,1:], axis=0)\n",
    "\n",
    "pos_data = pos[:,1:] - pos_mean\n",
    "neg_data = neg[:,1:] - neg_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the LDA algorithm here: [10 pts each for getting cov_all, w and y_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov_all = np.cov(np.concatenate((pos_data, neg_data), axis=0).T)\n",
    "w = np.linalg.solve(cov_all,(pos_mean - neg_mean))\n",
    "y_lda = np.transpose(w).dot(np.transpose(X[:,1:]- np.kron(np.ones((len(X), 1)),(1/2*(pos_mean + neg_mean)))))\n",
    "y_lda = y_lda > 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completing the code to compute the training set accuracy. You should get a training accuracy around 89%. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = 1 - np.sum(np.abs(y_lda.reshape((100,1)) - y))/ len(y);\n",
    "accuracy = accuracy * 100;\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Problem\n",
    "\n",
    "Show that the log-odds decision function a(x) for LDA\n",
    "\n",
    "$a = \\ln \\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}$\n",
    "\n",
    "is linear in x, that is, we can express $a(x)=\\theta^Tx$ for some $\\theta$. Show all your steps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER:\n",
    "\n",
    "[comment]: <> (your answer below here.)\n",
    "\n",
    "![solution](data/solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN on MNIST using TensorFlow&trade; [50 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Adapted from official TensorFlow&trade; tour guide.\n",
    "\n",
    "TensorFlow is a powerful library for doing large-scale numerical computation. One of the tasks at which it excels is implementing and training deep neural networks. In this assignment you will learn the basic building blocks of a TensorFlow model while constructing a deep convolutional MNIST classifier.\n",
    "\n",
    "What you are expected to implement in this tutorial:\n",
    "\n",
    "* Create a softmax regression function that is a model for recognizing MNIST digits, based on looking at every pixel in the image\n",
    "\n",
    "* Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples\n",
    "\n",
    "* Check the model's accuracy with MNIST test data\n",
    "\n",
    "* Build, train, and test a multilayer convolutional neural network to improve the results\n",
    "\n",
    "Here is a diagram, created with TensorBoard, of the model we will build:\n",
    "\n",
    "![tensorflow graph](data/graph.png)\n",
    "\n",
    "## Implement Utilities\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\". Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling [5 pts]\n",
    "\n",
    "Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input. Our pooling is plain old max pooling over 2x2 blocks.\n",
    "\n",
    "\n",
    "NOTE: FOR ALL THE FOLLOWING CODES, DO NOT IMPLEMENT YOUR OWN VERSION. USE THE BUILT-IN METHODS FROM TENSORFLOW.\n",
    "\n",
    "Take a look at [TensorFlow API Docs](https://www.tensorflow.org/api_docs/python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN\n",
    "\n",
    "### First Convolutional Layer[10 pts]\n",
    "\n",
    "We can now implement our first layer. It will consist of convolution, followed by max pooling. The convolution will compute 32 features for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel.\n",
    "\n",
    "To apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels.\n",
    "\n",
    "We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool. The max_pool_2x2 method will reduce the image size to 14x14.\n",
    "\n",
    "### Second Convolutional Layer[5 pts]\n",
    "\n",
    "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch.\n",
    "\n",
    "### Fully Connected Layer[10 pts]\n",
    "\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU.\n",
    "\n",
    "### SoftmaxLayer[5 pts]\n",
    "\n",
    "Finally, we add a layer of softmax regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    \"\"\"\n",
    "    deepnn builds the graph for a deep net for classifying digits.\n",
    "    Args:\n",
    "      x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "      number of pixels in a standard MNIST image.\n",
    "    Returns:\n",
    "      A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "      equal to the logits of classifying the digit into one of 10 classes (the\n",
    "      digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "      dropout.\n",
    "    \"\"\"\n",
    "    # Reshape to use within a convolutional neural net.\n",
    "    # Last dimension is for \"features\" - there is only one here, since images are\n",
    "    # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "    with tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "        b_conv1 = bias_variable([32])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    with tf.name_scope('pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "        b_conv2 = bias_variable([64])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    # Second pooling layer.\n",
    "    with tf.name_scope('pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "    # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        b_fc1 = bias_variable([1024])\n",
    "\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Map the 1024 features to 10 classes, one for each digit\n",
    "    with tf.name_scope('fc2'):\n",
    "        W_fc2 = weight_variable([1024, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "\n",
    "        y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    return y_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the Graph[10 pts]\n",
    "We start building the computation graph by creating nodes for the input images and target output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_conv = deepnn(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a loss function just as easily. Loss indicates how bad the model's prediction was on a single example; we try to minimize that while training across all the examples. Here, our loss function is the cross-entropy between the target and the softmax activation function applied to the model's prediction. As in the beginners tutorial, we use the stable formulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll figure out where we predicted the correct label. tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y\\_,1) is the true label. We can use tf.equal to check if our prediction matches the truth.\n",
    "\n",
    "That gives us a list of booleans. To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)    \n",
    "    accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving graph to: /tmp/tmptetx2a70\n"
     ]
    }
   ],
   "source": [
    "# For saving the graph, DO NOT CHANGE.\n",
    "graph_location = tempfile.mkdtemp()\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Model[5 pts]\n",
    "\n",
    "We will a more sophisticated ADAM optimizer instead of a Gradient Descent Optimizer.\n",
    "\n",
    "We will add logging to every 100th iteration in the training process.\n",
    "\n",
    "Feel free to run this code. Be aware that it does 20,000 training iterations and may take a while (possibly up to half an hour), depending on your processor.\n",
    "\n",
    "The final test set accuracy after running this code should be approximately 99.2%  -- not state of the art, but respectable.\n",
    "\n",
    "We have learned how to quickly and easily build, train, and evaluate a fairly sophisticated deep learning model using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.1\n",
      "step 500, training accuracy 0.94\n",
      "step 1000, training accuracy 0.96\n",
      "step 1500, training accuracy 0.98\n",
      "step 2000, training accuracy 0.96\n",
      "step 2500, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3500, training accuracy 1\n",
      "step 4000, training accuracy 0.98\n",
      "step 4500, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5500, training accuracy 0.98\n",
      "step 6000, training accuracy 0.98\n",
      "step 6500, training accuracy 0.98\n",
      "step 7000, training accuracy 1\n",
      "step 7500, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10500, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11500, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12500, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13500, training accuracy 0.98\n",
      "step 14000, training accuracy 1\n",
      "step 14500, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15500, training accuracy 0.98\n",
      "step 16000, training accuracy 1\n",
      "step 16500, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17500, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18500, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19500, training accuracy 1\n",
      "test accuracy 0.9922\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i % 500 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
