{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Number of Comments for Blogs ##\n",
    "\n",
    "In this assignment, we will work on blog posts. The deadline is ** Nov. 19 5pm **. The goal is to determine the number of comments a post will receive in the next 24 hours. You can find the details of the dataset [here](https://drive.google.com/a/bu.edu/file/d/19p7QBjH2Ai37MJcixiQ5UKlC0kw9l2Yd/view?usp=sharing). The data are stored in .csv files and each row of the dataset corresponds to a distinct blog (data instance). \n",
    "\n",
    "For information regarding the dataset and the features included in it please refer to the included README.md file.\n",
    "\n",
    "As learned in class most algorithms can only handle numeric values so we provided a dataset containing numeric values. The algorithms you are going to evaluate are the following: 1) Linear Regression, 2) Logistic Regression, 3) KNN, 4) Decision Tree Classifiers. Tasks 1 and 2 are related to regression to analyze this information, while tasks 3 and 4 are related to classification.\n",
    "\n",
    "Relevant Papers/citations:\n",
    "1) Buza, K. (2014). Feedback Prediction for Blogs. In Data Analysis, Machine Learning and Knowledge Discovery (pp. 145-152). Springer International Publishing.\n",
    "2) BlogFeedback Data Set UCI Machine Learning Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CJ/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "First, we will fit a linear regression model that predicts the number of comments a post will receive. Use the model to analyze the important factors that decide the number of comments of a blog. \n",
    "\n",
    "1. Report the results of your linear regression model (error, factors, e.t.c.) for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) how did you address overfitting, b) interpretation of the linear regression model results, c) compare the results for the two test datasets and mention any interesting observations ** (5 points) **\n",
    "\n",
    "Total: ** (15 points) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    281   R-squared:                       0.365\n",
      "Model:                            OLS   Adj. R-squared:                  0.362\n",
      "Method:                 Least Squares   F-statistic:                     120.7\n",
      "Date:                Sun, 19 Nov 2017   Prob (F-statistic):               0.00\n",
      "Time:                        15:14:17   Log-Likelihood:            -2.5265e+05\n",
      "No. Observations:               52396   AIC:                         5.058e+05\n",
      "Df Residuals:                   52147   BIC:                         5.080e+05\n",
      "Df Model:                         248                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "1             -0.6553      0.227     -2.883      0.004      -1.101      -0.210\n",
      "2              0.3549      0.184      1.933      0.053      -0.005       0.715\n",
      "3           -802.2068    584.833     -1.372      0.170   -1948.485     344.071\n",
      "4             -0.0059      0.007     -0.898      0.369      -0.019       0.007\n",
      "5              0.0685      0.073      0.941      0.347      -0.074       0.211\n",
      "6          -6.909e+05   3.61e+05     -1.911      0.056    -1.4e+06    1.76e+04\n",
      "7             -0.7753      0.421     -1.840      0.066      -1.601       0.050\n",
      "8             -1.9033      1.761     -1.081      0.280      -5.354       1.548\n",
      "9              0.0361      0.023      1.541      0.123      -0.010       0.082\n",
      "10            -0.1717      0.184     -0.932      0.351      -0.532       0.189\n",
      "11          6.909e+05   3.61e+05      1.911      0.056   -1.76e+04     1.4e+06\n",
      "12             0.5346      0.285      1.875      0.061      -0.024       1.093\n",
      "13           -53.1871     38.748     -1.373      0.170    -129.133      22.759\n",
      "14            -0.0609      0.027     -2.240      0.025      -0.114      -0.008\n",
      "15             0.2918      0.229      1.277      0.202      -0.156       0.740\n",
      "16             0.1849      0.213      0.869      0.385      -0.232       0.602\n",
      "17            -0.2935      0.135     -2.170      0.030      -0.558      -0.028\n",
      "18           802.1962    584.828      1.372      0.170    -344.072    1948.464\n",
      "19             0.0162      0.009      1.899      0.058      -0.001       0.033\n",
      "20             0.0168      0.079      0.212      0.832      -0.138       0.172\n",
      "21          6.909e+05   3.61e+05      1.911      0.056   -1.76e+04     1.4e+06\n",
      "22             0.1577      0.183      0.861      0.389      -0.201       0.517\n",
      "23            -0.0090      0.007     -1.288      0.198      -0.023       0.005\n",
      "24             0.0034      0.012      0.283      0.777      -0.020       0.027\n",
      "25             0.2000      0.074      2.712      0.007       0.055       0.345\n",
      "26            -5.7616      9.886     -0.583      0.560     -25.139      13.615\n",
      "27             0.5741      6.197      0.093      0.926     -11.572      12.720\n",
      "28             0.0598      1.495      0.040      0.968      -2.870       2.990\n",
      "29            -0.2625      0.385     -0.682      0.495      -1.017       0.492\n",
      "30            -0.0949      1.067     -0.089      0.929      -2.187       1.997\n",
      "31         -2.236e+07   1.63e+07     -1.372      0.170   -5.43e+07    9.59e+06\n",
      "32            11.2907     16.882      0.669      0.504     -21.798      44.379\n",
      "33             0.5615      0.409      1.372      0.170      -0.241       1.364\n",
      "34            -0.4940      1.767     -0.280      0.780      -3.957       2.969\n",
      "35             3.8244      5.899      0.648      0.517      -7.738      15.386\n",
      "36          2.236e+07   1.63e+07      1.372      0.170   -9.59e+06    5.43e+07\n",
      "37           -13.7410     16.084     -0.854      0.393     -45.265      17.783\n",
      "38            -0.4110      0.298     -1.378      0.168      -0.995       0.173\n",
      "39             1.1475      1.809      0.634      0.526      -2.398       4.693\n",
      "40            -7.9951     13.478     -0.593      0.553     -34.412      18.422\n",
      "41             3.0523      8.749      0.349      0.727     -14.095      20.200\n",
      "42             0.6843      5.478      0.125      0.901     -10.053      11.421\n",
      "43             0.6472      1.418      0.456      0.648      -2.133       3.427\n",
      "44             0.1836      0.366      0.501      0.616      -0.534       0.902\n",
      "45            -0.1864      1.152     -0.162      0.871      -2.445       2.072\n",
      "46          2.236e+07   1.63e+07      1.372      0.170   -9.59e+06    5.43e+07\n",
      "47            -1.0418      7.799     -0.134      0.894     -16.328      14.244\n",
      "48             0.0219      0.636      0.035      0.972      -1.224       1.268\n",
      "49            -0.4325      0.611     -0.708      0.479      -1.630       0.765\n",
      "50             0.8489     12.112      0.070      0.944     -22.890      24.588\n",
      "51            -0.0421      0.007     -6.162      0.000      -0.055      -0.029\n",
      "52             4.0373      2.856      1.413      0.158      -1.561       9.636\n",
      "53            -3.8610      2.856     -1.352      0.176      -9.460       1.738\n",
      "54            -0.0312      0.007     -4.446      0.000      -0.045      -0.017\n",
      "55            -3.8207      2.856     -1.338      0.181      -9.419       1.778\n",
      "56            -0.8940      0.554     -1.615      0.106      -1.979       0.191\n",
      "57             0.5566      0.275      2.026      0.043       0.018       1.095\n",
      "58             0.1955      0.279      0.699      0.484      -0.352       0.743\n",
      "59             0.9088      0.543      1.675      0.094      -0.155       1.972\n",
      "60            -0.1021      0.144     -0.711      0.477      -0.384       0.179\n",
      "61            -0.1514      0.007    -21.820      0.000      -0.165      -0.138\n",
      "62             0.0001   7.07e-05      1.736      0.083   -1.59e-05       0.000\n",
      "63             2.0868      3.401      0.614      0.539      -4.578       8.752\n",
      "64            -0.5859      0.397     -1.474      0.140      -1.365       0.193\n",
      "65             0.0055      3.134      0.002      0.999      -6.137       6.148\n",
      "66             0.9145      8.777      0.104      0.917     -16.288      18.117\n",
      "67            -0.3768      0.371     -1.017      0.309      -1.103       0.350\n",
      "68             0.3475      0.416      0.835      0.404      -0.468       1.163\n",
      "69             1.5218      0.370      4.110      0.000       0.796       2.248\n",
      "70            -2.8893      2.752     -1.050      0.294      -8.284       2.505\n",
      "71            -2.4805      2.343     -1.059      0.290      -7.073       2.112\n",
      "72             2.5638      0.864      2.967      0.003       0.870       4.257\n",
      "73             0.4498      3.128      0.144      0.886      -5.681       6.580\n",
      "74            -6.4819      5.725     -1.132      0.258     -17.704       4.740\n",
      "75            -1.2871      2.058     -0.625      0.532      -5.322       2.748\n",
      "76             0.2558      3.126      0.082      0.935      -5.872       6.384\n",
      "77             4.7881      1.195      4.007      0.000       2.446       7.130\n",
      "78            -2.2797      3.640     -0.626      0.531      -9.413       4.854\n",
      "79            -0.0959      0.380     -0.252      0.801      -0.841       0.649\n",
      "80             0.5958      3.133      0.190      0.849      -5.545       6.737\n",
      "81             0.2797      3.126      0.089      0.929      -5.848       6.407\n",
      "82            -4.3700      2.427     -1.800      0.072      -9.127       0.387\n",
      "83            -0.7627      3.553     -0.215      0.830      -7.727       6.202\n",
      "84           -12.3931     12.412     -0.998      0.318     -36.721      11.935\n",
      "85            29.8981      8.943      3.343      0.001      12.370      47.426\n",
      "86            -0.1069      0.951     -0.112      0.911      -1.972       1.758\n",
      "87           -15.3733     12.831     -1.198      0.231     -40.521       9.775\n",
      "88             2.7050      1.265      2.138      0.032       0.226       5.184\n",
      "89            -1.1480      0.578     -1.985      0.047      -2.282      -0.014\n",
      "90            30.2844     17.498      1.731      0.084      -4.012      64.581\n",
      "91            -5.6561      8.925     -0.634      0.526     -23.148      11.836\n",
      "92            29.9561     10.106      2.964      0.003      10.148      49.764\n",
      "93             1.4415      1.959      0.736      0.462      -2.398       5.281\n",
      "94             0.5531      3.131      0.177      0.860      -5.584       6.690\n",
      "95            -6.6310     12.367     -0.536      0.592     -30.870      17.608\n",
      "96             0.9502      0.498      1.909      0.056      -0.025       1.926\n",
      "97             0.5271      3.130      0.168      0.866      -5.608       6.662\n",
      "98             9.8975      7.396      1.338      0.181      -4.600      24.395\n",
      "99            -2.9611      7.915     -0.374      0.708     -18.474      12.552\n",
      "100           -5.6110      4.438     -1.264      0.206     -14.310       3.088\n",
      "101           -0.4789      0.455     -1.052      0.293      -1.371       0.413\n",
      "102            0.7571      0.529      1.433      0.152      -0.279       1.793\n",
      "103            0.0439      0.801      0.055      0.956      -1.526       1.614\n",
      "104            0.6704      0.669      1.002      0.316      -0.641       1.981\n",
      "105            2.0418      2.487      0.821      0.412      -2.834       6.917\n",
      "106           -1.6058      3.892     -0.413      0.680      -9.235       6.023\n",
      "107           -4.2433      2.555     -1.661      0.097      -9.251       0.764\n",
      "108           -1.4382      1.082     -1.329      0.184      -3.559       0.683\n",
      "109            0.3036     17.522      0.017      0.986     -34.040      34.647\n",
      "110           -1.3620      0.869     -1.568      0.117      -3.065       0.341\n",
      "111            0.5981     12.340      0.048      0.961     -23.588      24.785\n",
      "112           -3.6173      7.176     -0.504      0.614     -17.682      10.448\n",
      "113            2.9714      8.912      0.333      0.739     -14.496      20.438\n",
      "114           -1.0885      0.420     -2.594      0.010      -1.911      -0.266\n",
      "115           -0.1072      0.559     -0.192      0.848      -1.202       0.988\n",
      "116           -1.7358      0.952     -1.824      0.068      -3.601       0.130\n",
      "117           -1.4490      3.399     -0.426      0.670      -8.112       5.214\n",
      "118            0.7997      1.246      0.642      0.521      -1.643       3.243\n",
      "119           -1.2384      0.646     -1.916      0.055      -2.505       0.028\n",
      "120           -0.2330      0.407     -0.573      0.567      -1.030       0.564\n",
      "121            0.7254      0.741      0.978      0.328      -0.728       2.178\n",
      "122           -0.4105      0.370     -1.110      0.267      -1.135       0.314\n",
      "123           -1.7789      7.026     -0.253      0.800     -15.549      11.991\n",
      "124           22.8617     18.005      1.270      0.204     -12.428      58.152\n",
      "125           -1.6132      1.349     -1.196      0.232      -4.257       1.031\n",
      "126            4.8826      6.937      0.704      0.482      -8.715      18.480\n",
      "127           -3.0921      1.818     -1.700      0.089      -6.656       0.472\n",
      "128            0.1858      1.225      0.152      0.879      -2.215       2.586\n",
      "129            1.1610     17.429      0.067      0.947     -32.999      35.321\n",
      "130            0.2469      3.127      0.079      0.937      -5.881       6.375\n",
      "131           -0.9134      1.958     -0.466      0.641      -4.751       2.924\n",
      "132            0.3959      3.127      0.127      0.899      -5.733       6.524\n",
      "133            3.6876      6.040      0.610      0.542      -8.152      15.527\n",
      "134            0.3722      0.724      0.514      0.607      -1.046       1.791\n",
      "135           -0.9514      0.846     -1.125      0.261      -2.609       0.707\n",
      "136            0.2219      0.661      0.336      0.737      -1.074       1.517\n",
      "137            4.6389      2.218      2.092      0.036       0.292       8.986\n",
      "138            1.4093      0.804      1.754      0.080      -0.166       2.985\n",
      "139            0.2073      0.372      0.557      0.578      -0.522       0.937\n",
      "140           -0.2522      0.532     -0.474      0.636      -1.296       0.791\n",
      "141            0.5329      0.537      0.993      0.321      -0.519       1.585\n",
      "142            1.0898      2.748      0.397      0.692      -4.297       6.476\n",
      "143           -0.6962      0.536     -1.299      0.194      -1.747       0.355\n",
      "144            0.9709      1.529      0.635      0.525      -2.026       3.968\n",
      "145           -5.5777      2.419     -2.306      0.021     -10.318      -0.837\n",
      "146           -2.7351      2.447     -1.118      0.264      -7.531       2.060\n",
      "147           -7.0784      1.910     -3.706      0.000     -10.822      -3.334\n",
      "148           -6.0704     10.373     -0.585      0.558     -26.401      14.260\n",
      "149            0.4013      3.127      0.128      0.898      -5.727       6.530\n",
      "150            1.0276      2.873      0.358      0.721      -4.604       6.659\n",
      "151            0.4490      0.353      1.273      0.203      -0.242       1.140\n",
      "152           -3.1308      2.997     -1.045      0.296      -9.004       2.743\n",
      "153           -1.0497      0.651     -1.613      0.107      -2.325       0.226\n",
      "154            2.1827      0.781      2.794      0.005       0.652       3.714\n",
      "155          -18.1666     17.473     -1.040      0.298     -52.414      16.081\n",
      "156            0.3061      3.126      0.098      0.922      -5.821       6.433\n",
      "157            0.3000      1.472      0.204      0.838      -2.585       3.185\n",
      "158            1.0377      0.362      2.865      0.004       0.328       1.748\n",
      "159            0.8268      0.478      1.730      0.084      -0.110       1.764\n",
      "160           -7.1303      4.158     -1.715      0.086     -15.279       1.018\n",
      "161            0.3650      3.126      0.117      0.907      -5.763       6.493\n",
      "162           -0.5814      0.817     -0.712      0.476      -2.182       1.019\n",
      "163            0.7349      0.855      0.859      0.390      -0.942       2.411\n",
      "164           -2.2388      1.818     -1.231      0.218      -5.803       1.325\n",
      "165           -0.6355      3.845     -0.165      0.869      -8.171       6.900\n",
      "166            0.3018      3.126      0.097      0.923      -5.825       6.429\n",
      "167            1.2390      3.879      0.319      0.749      -6.365       8.843\n",
      "168           -2.3841      9.609     -0.248      0.804     -21.218      16.450\n",
      "169            0.3604      3.126      0.115      0.908      -5.767       6.488\n",
      "170           -0.5485      0.402     -1.363      0.173      -1.337       0.240\n",
      "171           16.3469      4.915      3.326      0.001       6.713      25.981\n",
      "172            0.3237      3.126      0.104      0.918      -5.804       6.451\n",
      "173           -4.3954      8.776     -0.501      0.616     -21.597      12.806\n",
      "174           -1.5348      1.145     -1.340      0.180      -3.779       0.710\n",
      "175            1.4057      2.618      0.537      0.591      -3.726       6.538\n",
      "176           -4.8284      4.141     -1.166      0.244     -12.944       3.287\n",
      "177           -1.0940      2.231     -0.490      0.624      -5.467       3.279\n",
      "178           -1.5933      7.149     -0.223      0.824     -15.605      12.418\n",
      "179            0.3764      3.127      0.120      0.904      -5.752       6.504\n",
      "180           -0.3892      0.494     -0.788      0.431      -1.358       0.579\n",
      "181           -0.6738      0.690     -0.976      0.329      -2.026       0.679\n",
      "182            0.5059      1.370      0.369      0.712      -2.180       3.192\n",
      "183           -0.7029      0.546     -1.287      0.198      -1.773       0.367\n",
      "184            0.6831      0.471      1.450      0.147      -0.240       1.607\n",
      "185            0.2853      7.886      0.036      0.971     -15.171      15.742\n",
      "186            1.2217      1.155      1.058      0.290      -1.042       3.486\n",
      "187           -1.1736      0.997     -1.177      0.239      -3.128       0.781\n",
      "188           -1.9178      1.798     -1.067      0.286      -5.442       1.606\n",
      "189           -0.9919      1.130     -0.878      0.380      -3.207       1.223\n",
      "190            0.3238      3.126      0.104      0.918      -5.804       6.451\n",
      "191           -0.3787      0.467     -0.812      0.417      -1.293       0.536\n",
      "192            1.0289      1.082      0.951      0.342      -1.092       3.150\n",
      "193            0.0428      0.437      0.098      0.922      -0.813       0.899\n",
      "194           -0.7057      0.414     -1.704      0.088      -1.518       0.106\n",
      "195            2.7967      0.960      2.912      0.004       0.914       4.679\n",
      "196           -1.4933      1.050     -1.423      0.155      -3.551       0.564\n",
      "197           -0.9807      0.543     -1.804      0.071      -2.046       0.085\n",
      "198            0.3511      3.126      0.112      0.911      -5.776       6.479\n",
      "199            3.9417      8.860      0.445      0.656     -13.423      21.307\n",
      "200            0.3147      3.126      0.101      0.920      -5.813       6.442\n",
      "201           -2.3503      2.200     -1.068      0.285      -6.662       1.962\n",
      "202            0.6355      0.404      1.572      0.116      -0.157       1.428\n",
      "203           -4.2086      4.906     -0.858      0.391     -13.824       5.407\n",
      "204            2.5855      3.758      0.688      0.491      -4.780       9.951\n",
      "205            0.9221      1.338      0.689      0.491      -1.701       3.545\n",
      "206           -1.2385      2.104     -0.589      0.556      -5.362       2.885\n",
      "207            0.3870      0.650      0.596      0.551      -0.887       1.661\n",
      "208            1.7942      1.253      1.431      0.152      -0.662       4.251\n",
      "209           -1.4602      3.999     -0.365      0.715      -9.298       6.378\n",
      "210            0.2502      0.364      0.687      0.492      -0.464       0.964\n",
      "211           -3.0000      2.707     -1.108      0.268      -8.305       2.305\n",
      "212            0.3431      3.126      0.110      0.913      -5.784       6.471\n",
      "213            0.0932      0.376      0.248      0.804      -0.643       0.829\n",
      "214            5.5534      6.615      0.839      0.401      -7.413      18.520\n",
      "215           -0.2754      2.138     -0.129      0.898      -4.467       3.916\n",
      "216            1.1070      0.572      1.935      0.053      -0.014       2.228\n",
      "217          -29.4940      5.879     -5.017      0.000     -41.017     -17.971\n",
      "218            5.1333      2.928      1.753      0.080      -0.605      10.872\n",
      "219           -2.1354      0.835     -2.556      0.011      -3.773      -0.498\n",
      "220            0.0567      3.254      0.017      0.986      -6.321       6.434\n",
      "221            1.8634      0.690      2.699      0.007       0.510       3.217\n",
      "222          -10.4006      3.675     -2.830      0.005     -17.604      -3.197\n",
      "223           -1.5779     18.208     -0.087      0.931     -37.266      34.110\n",
      "224           -4.3501      8.854     -0.491      0.623     -21.704      13.003\n",
      "225           -2.0863      1.687     -1.237      0.216      -5.393       1.221\n",
      "226            1.2547      0.380      3.300      0.001       0.510       2.000\n",
      "227            0.8909      1.876      0.475      0.635      -2.785       4.567\n",
      "228           -0.3573      0.429     -0.834      0.405      -1.198       0.483\n",
      "229           -0.7691      3.471     -0.222      0.825      -7.573       6.035\n",
      "230            0.2237      0.446      0.502      0.616      -0.651       1.098\n",
      "231           -1.8271      2.087     -0.875      0.381      -5.919       2.264\n",
      "232            0.7406      0.362      2.048      0.041       0.032       1.449\n",
      "233            0.0277      0.408      0.068      0.946      -0.773       0.828\n",
      "234           -0.5926      0.701     -0.846      0.398      -1.966       0.781\n",
      "235           -1.3905      3.718     -0.374      0.708      -8.679       5.898\n",
      "236            0.3378      3.126      0.108      0.914      -5.790       6.465\n",
      "237           -2.6716      1.905     -1.402      0.161      -6.406       1.063\n",
      "238           -3.1699      2.885     -1.099      0.272      -8.825       2.485\n",
      "239           -0.0207      4.623     -0.004      0.996      -9.083       9.041\n",
      "240           -0.7265      3.193     -0.228      0.820      -6.984       5.531\n",
      "241            2.7659      0.839      3.295      0.001       1.121       4.411\n",
      "242           -2.1271      1.169     -1.820      0.069      -4.417       0.163\n",
      "243            0.3338      3.126      0.107      0.915      -5.794       6.461\n",
      "244            0.3328     17.678      0.019      0.985     -34.317      34.982\n",
      "245            0.1578      0.964      0.164      0.870      -1.732       2.048\n",
      "246           -0.7203      0.526     -1.370      0.171      -1.751       0.310\n",
      "247           -0.8099      0.443     -1.829      0.067      -1.678       0.058\n",
      "248            0.4746      0.377      1.257      0.209      -0.265       1.215\n",
      "249           -0.0834      1.606     -0.052      0.959      -3.231       3.064\n",
      "250            0.3383      3.126      0.108      0.914      -5.789       6.466\n",
      "251           -0.5542      1.119     -0.495      0.621      -2.748       1.640\n",
      "252            3.0399      1.697      1.792      0.073      -0.285       6.365\n",
      "253            0.1097     12.410      0.009      0.993     -24.213      24.433\n",
      "254           -0.2013      0.868     -0.232      0.817      -1.904       1.501\n",
      "255           -1.7625      1.224     -1.440      0.150      -4.162       0.637\n",
      "256            2.5716     10.201      0.252      0.801     -17.422      22.565\n",
      "257            0.9625      0.716      1.345      0.179      -0.440       2.365\n",
      "258            0.4220      2.857      0.148      0.883      -5.177       6.021\n",
      "259            0.7388      2.325      0.318      0.751      -3.818       5.295\n",
      "260            3.8657      2.653      1.457      0.145      -1.335       9.066\n",
      "261            2.8867      1.484      1.945      0.052      -0.022       5.795\n",
      "262           -2.4645      8.739     -0.282      0.778     -19.594      14.665\n",
      "263            3.5263      0.491      7.186      0.000       2.564       4.488\n",
      "264            2.4340      0.465      5.239      0.000       1.523       3.345\n",
      "265            1.6067      0.441      3.645      0.000       0.743       2.471\n",
      "266            2.0205      0.417      4.847      0.000       1.203       2.838\n",
      "267            1.7728      0.420      4.224      0.000       0.950       2.595\n",
      "268            2.4858      0.435      5.716      0.000       1.633       3.338\n",
      "269            3.8271      0.469      8.155      0.000       2.907       4.747\n",
      "270            2.2596      0.457      4.947      0.000       1.364       3.155\n",
      "271            3.0540      0.441      6.920      0.000       2.189       3.919\n",
      "272            2.5427      0.425      5.987      0.000       1.710       3.375\n",
      "273            1.8563      0.427      4.351      0.000       1.020       2.692\n",
      "274            2.0229      0.436      4.642      0.000       1.169       2.877\n",
      "275            2.5744      0.497      5.181      0.000       1.600       3.548\n",
      "276            2.2577      0.516      4.379      0.000       1.247       3.268\n",
      "277            0.0275      0.093      0.297      0.767      -0.155       0.210\n",
      "278                 0          0        nan        nan           0           0\n",
      "279           -0.0025      0.011     -0.237      0.813      -0.023       0.018\n",
      "280            0.0062      0.014      0.434      0.664      -0.022       0.034\n",
      "==============================================================================\n",
      "Omnibus:                    85796.406   Durbin-Watson:                   1.925\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        149954782.366\n",
      "Skew:                          10.591   Prob(JB):                         0.00\n",
      "Kurtosis:                     264.224   Cond. No.                     1.22e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is  8e-21. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CJ/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:1036: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "df_02 = pd.read_csv('blogData_test-2012.02.csv')\n",
    "df_02.columns = [str(i) for i in range(1,282)]\n",
    "\n",
    "df_03 = pd.read_csv('blogData_test-2012.03.csv')\n",
    "df_03.columns = [str(i) for i in range(1,282)]\n",
    "\n",
    "df_train = pd.read_csv('blogData_train.csv')\n",
    "df_train.columns = [str(i) for i in range(1,282)]\n",
    "\n",
    "df_train_X = df_train.iloc[ :, :-1]\n",
    "df_train_y = df_train.iloc[ :, -1:]\n",
    "\n",
    "# Variable selection running full model\n",
    "train_model = sm.OLS(df_train_y,df_train_X)\n",
    "train_results = train_model.fit()\n",
    "print(train_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Intervals:                 0             1\n",
      "1   -1.100943e+00 -2.097544e-01\n",
      "2   -5.029501e-03  7.147765e-01\n",
      "3   -1.948485e+03  3.440709e+02\n",
      "4   -1.872786e-02  6.957279e-03\n",
      "5   -7.418628e-02  2.112173e-01\n",
      "6   -1.399361e+06  1.763764e+04\n",
      "7   -1.601004e+00  5.048369e-02\n",
      "8   -5.354273e+00  1.547752e+00\n",
      "9   -9.818360e-03  8.207265e-02\n",
      "10  -5.324405e-01  1.891404e-01\n",
      "11  -1.763619e+04  1.399363e+06\n",
      "12  -2.422854e-02  1.093428e+00\n",
      "13  -1.291327e+02  2.275861e+01\n",
      "14  -1.142064e-01 -7.613162e-03\n",
      "15  -1.561071e-01  7.396886e-01\n",
      "16  -2.323271e-01  6.020761e-01\n",
      "17  -5.584776e-01 -2.843401e-02\n",
      "18  -3.440717e+02  1.948464e+03\n",
      "19  -5.217820e-04  3.297789e-02\n",
      "20  -1.381059e-01  1.716907e-01\n",
      "21  -1.763509e+04  1.399364e+06\n",
      "22  -2.013264e-01  5.166344e-01\n",
      "23  -2.261255e-02  4.681406e-03\n",
      "24  -2.029998e-02  2.715765e-02\n",
      "25   5.545269e-02  3.446170e-01\n",
      "26  -2.513853e+01  1.361526e+01\n",
      "27  -1.157163e+01  1.271988e+01\n",
      "28  -2.870102e+00  2.989696e+00\n",
      "29  -1.017057e+00  4.921518e-01\n",
      "30  -2.186859e+00  1.997064e+00\n",
      "..            ...           ...\n",
      "251 -2.747818e+00  1.639503e+00\n",
      "252 -2.852484e-01  6.365123e+00\n",
      "253 -2.421347e+01  2.443280e+01\n",
      "254 -1.903527e+00  1.500919e+00\n",
      "255 -4.162093e+00  6.370444e-01\n",
      "256 -1.742180e+01  2.256504e+01\n",
      "257 -4.402910e-01  2.365206e+00\n",
      "258 -5.176833e+00  6.020780e+00\n",
      "259 -3.817897e+00  5.295445e+00\n",
      "260 -1.334658e+00  9.065969e+00\n",
      "261 -2.188947e-02  5.795335e+00\n",
      "262 -1.959389e+01  1.466480e+01\n",
      "263  2.564477e+00  4.488042e+00\n",
      "264  1.523359e+00  3.344598e+00\n",
      "265  7.426701e-01  2.470674e+00\n",
      "266  1.203461e+00  2.837626e+00\n",
      "267  9.502508e-01  2.595263e+00\n",
      "268  1.633405e+00  3.338277e+00\n",
      "269  2.907234e+00  4.746952e+00\n",
      "270  1.364386e+00  3.154814e+00\n",
      "271  2.188934e+00  3.919026e+00\n",
      "272  1.710244e+00  3.375147e+00\n",
      "273  1.020139e+00  2.692500e+00\n",
      "274  1.168812e+00  2.876993e+00\n",
      "275  1.600463e+00  3.548437e+00\n",
      "276  1.247121e+00  3.268246e+00\n",
      "277 -1.545388e-01  2.096354e-01\n",
      "278  0.000000e+00  0.000000e+00\n",
      "279 -2.325588e-02  1.824015e-02\n",
      "280 -2.184153e-02  3.427920e-02\n",
      "\n",
      "[280 rows x 2 columns]\n",
      "Parameters: 1          -0.655349\n",
      "2           0.354874\n",
      "3        -802.206816\n",
      "4          -0.005885\n",
      "5           0.068516\n",
      "6     -690861.878757\n",
      "7          -0.775260\n",
      "8          -1.903261\n",
      "9           0.036127\n",
      "10         -0.171650\n",
      "11     690863.155364\n",
      "12          0.534600\n",
      "13        -53.187068\n",
      "14         -0.060910\n",
      "15          0.291791\n",
      "16          0.184875\n",
      "17         -0.293456\n",
      "18        802.196191\n",
      "19          0.016228\n",
      "20          0.016792\n",
      "21     690864.397865\n",
      "22          0.157654\n",
      "23         -0.008966\n",
      "24          0.003429\n",
      "25          0.200035\n",
      "26         -5.761634\n",
      "27          0.574124\n",
      "28          0.059797\n",
      "29         -0.262452\n",
      "30         -0.094898\n",
      "           ...      \n",
      "251        -0.554157\n",
      "252         3.039937\n",
      "253         0.109666\n",
      "254        -0.201304\n",
      "255        -1.762525\n",
      "256         2.571620\n",
      "257         0.962457\n",
      "258         0.421973\n",
      "259         0.738774\n",
      "260         3.865655\n",
      "261         2.886723\n",
      "262        -2.464546\n",
      "263         3.526260\n",
      "264         2.433979\n",
      "265         1.606672\n",
      "266         2.020544\n",
      "267         1.772757\n",
      "268         2.485841\n",
      "269         3.827093\n",
      "270         2.259600\n",
      "271         3.053980\n",
      "272         2.542695\n",
      "273         1.856319\n",
      "274         2.022903\n",
      "275         2.574450\n",
      "276         2.257683\n",
      "277         0.027548\n",
      "278         0.000000\n",
      "279        -0.002508\n",
      "280         0.006219\n",
      "Length: 280, dtype: float64\n",
      "--------------------------------------------------------------------------------------------\n",
      "The dimension of Xsignif is:  (52396, 45) , indicating only 45 features/columns are retained\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get rid of these variables that are not statistically significant \n",
    "print('Confidence Intervals: {}'.format(train_results.conf_int()))\n",
    "print('Parameters: {}'.format(train_results.params))\n",
    "\n",
    "CIs = train_results.conf_int()\n",
    "notSignificant = (CIs.iloc[:,0] < 0) & (CIs.iloc[:,1] > 0)\n",
    "notSignificant = pd.DataFrame(notSignificant)\n",
    "Significant = ~notSignificant\n",
    "Xsignif = df_train_X.loc[ : , Significant[0]]\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"The dimension of Xsignif is: \", Xsignif.shape,\", indicating only 45 features/columns are retained\")\n",
    "print(\"--------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    281   R-squared:                       0.294\n",
      "Model:                            OLS   Adj. R-squared:                  0.293\n",
      "Method:                 Least Squares   F-statistic:                     518.6\n",
      "Date:                Sun, 19 Nov 2017   Prob (F-statistic):               0.00\n",
      "Time:                        15:14:29   Log-Likelihood:            -2.5542e+05\n",
      "No. Observations:               52396   AIC:                         5.109e+05\n",
      "Df Residuals:                   52353   BIC:                         5.113e+05\n",
      "Df Model:                          42                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "1              0.3377      0.004     79.519      0.000       0.329       0.346\n",
      "14            -0.0061      0.001     -5.870      0.000      -0.008      -0.004\n",
      "17            -0.1278      0.008    -16.154      0.000      -0.143      -0.112\n",
      "25             0.6139      0.046     13.460      0.000       0.525       0.703\n",
      "51            -0.0204      0.006     -3.439      0.001      -0.032      -0.009\n",
      "54            -0.0059      0.007     -0.857      0.392      -0.019       0.008\n",
      "57             5.5128      0.173     31.930      0.000       5.174       5.851\n",
      "61            -0.2462      0.007    -35.039      0.000      -0.260      -0.232\n",
      "69             1.4022      0.371      3.778      0.000       0.675       2.130\n",
      "72             0.0592      0.817      0.072      0.942      -1.541       1.660\n",
      "77             2.9025      1.173      2.475      0.013       0.604       5.201\n",
      "85            21.3216      8.588      2.483      0.013       4.489      38.154\n",
      "88             2.5971      1.302      1.994      0.046       0.045       5.149\n",
      "89            -1.6762      0.587     -2.854      0.004      -2.827      -0.525\n",
      "92            17.4242      9.395      1.855      0.064      -0.990      35.839\n",
      "114           -1.4056      0.424     -3.317      0.001      -2.236      -0.575\n",
      "137            3.9869      2.250      1.772      0.076      -0.422       8.396\n",
      "145           -5.1891      2.301     -2.255      0.024      -9.699      -0.679\n",
      "147           -7.8697      1.967     -4.002      0.000     -11.724      -4.015\n",
      "154            3.6279      0.800      4.536      0.000       2.060       5.196\n",
      "158            0.5037      0.350      1.437      0.151      -0.183       1.191\n",
      "171           19.5298      5.009      3.899      0.000       9.712      29.348\n",
      "195            3.0377      0.990      3.069      0.002       1.098       4.978\n",
      "217          -33.6597      5.893     -5.712      0.000     -45.210     -22.109\n",
      "219           -1.4673      0.863     -1.701      0.089      -3.158       0.223\n",
      "221            1.4555      0.709      2.053      0.040       0.066       2.845\n",
      "222          -14.3557      3.734     -3.845      0.000     -21.674      -7.037\n",
      "226            0.7515      0.354      2.126      0.034       0.059       1.444\n",
      "232            0.4750      0.343      1.385      0.166      -0.197       1.147\n",
      "241            2.0970      0.863      2.431      0.015       0.406       3.788\n",
      "263            5.3517      0.493     10.857      0.000       4.386       6.318\n",
      "264            4.7456      0.464     10.235      0.000       3.837       5.654\n",
      "265            3.5278      0.437      8.077      0.000       2.672       4.384\n",
      "266            3.6717      0.411      8.942      0.000       2.867       4.476\n",
      "267            3.1205      0.414      7.536      0.000       2.309       3.932\n",
      "268            3.6526      0.431      8.465      0.000       2.807       4.498\n",
      "269            5.0227      0.469     10.699      0.000       4.103       5.943\n",
      "270            3.6208      0.449      8.063      0.000       2.741       4.501\n",
      "271            4.7740      0.432     11.057      0.000       3.928       5.620\n",
      "272            4.4675      0.413     10.809      0.000       3.657       5.278\n",
      "273            3.6729      0.417      8.799      0.000       2.855       4.491\n",
      "274            4.1132      0.427      9.628      0.000       3.276       4.951\n",
      "275            4.5822      0.494      9.273      0.000       3.614       5.551\n",
      "276            3.8619      0.514      7.518      0.000       2.855       4.869\n",
      "278                 0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                    85105.963   Durbin-Watson:                   1.817\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        130705517.231\n",
      "Skew:                          10.462   Prob(JB):                         0.00\n",
      "Kurtosis:                     246.787   Cond. No.                     1.18e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 7.08e-23. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CJ/anaconda/lib/python3.6/site-packages/statsmodels/base/model.py:1036: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1818: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "# Construt model\n",
    "post_model = sm.OLS(df_train_y, Xsignif)\n",
    "post_results = post_model.fit()\n",
    "print(post_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Test blogData_test-2012.02 is: 756.050115665\n",
      "MSE of Test blogData_test-2012.03 is: 820.708588528\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAFPCAYAAACGU+LuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIm6ugyAIgiAIgqBN/D9FXMiMI1SNYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116af38d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select columns from testing set \n",
    "df_02_testing_set_X = df_02[list(Xsignif.columns)]\n",
    "df_03_testing_set_X = df_03[list(Xsignif.columns)]\n",
    "\n",
    "df_02_testing_y = df_02.iloc[ :, -1:]\n",
    "df_03_testing_y = df_03.iloc[ :, -1:]\n",
    "\n",
    "# 'blogData_test-2012.02 Prediction'\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,sharey = 'row',figsize=(12,5))\n",
    "y_oo2_predict = post_results.predict(df_02_testing_set_X)\n",
    "ax1.scatter(df_02_testing_y, y_oo2_predict)\n",
    "ax1.set_xlabel('True number of comments received in the next 24 hours')\n",
    "ax1.set_ylabel('Predicted number of comments received in the next 24 hours')\n",
    "ax1.plot([0,700],[0,700],'r-')\n",
    "ax1.axis('equal')\n",
    "ax1.set_ylim([0,700])\n",
    "ax1.set_xlim([0,700])\n",
    "ax1.set_title('blogData_test-2012.02 Prediction')\n",
    "\n",
    "\n",
    "# 'blogData_test-2012.03 Prediction'\n",
    "y_oo3_predict = post_results.predict(df_03_testing_set_X)\n",
    "ax2.scatter(df_03_testing_y,y_oo3_predict)\n",
    "ax2.set_xlabel('True number of comments received in the next 24 hours')\n",
    "ax2.set_ylabel('Predicted number of comments received in the next 24 hours')\n",
    "ax2.plot([0,700],[0,700],'r-')\n",
    "ax2.axis('equal')\n",
    "ax2.set_ylim([0,700])\n",
    "ax2.set_xlim([0,700])\n",
    "ax2.set_title('blogData_test-2012.03 Prediction')\n",
    "\n",
    "\n",
    "# More numerical approach\n",
    "\n",
    "test_02 = mean_squared_error(df_02_testing_y, y_oo2_predict)\n",
    "test_03 = mean_squared_error(df_03_testing_y, y_oo3_predict) \n",
    "\n",
    "#--------------------------------------------------------------\n",
    "print(\"MSE of Test blogData_test-2012.02 is: \" + str(test_02))\n",
    "print(\"MSE of Test blogData_test-2012.03 is: \" + str(test_03))\n",
    "#--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) How did you address overfitting\n",
    "\n",
    "Normally there are two ways to address overfitting: \n",
    "\n",
    "1) Rridge Regression + k-fold Cross-validation where we try to find the optimal lamda that minimizes MSE in the training set, and then use newly constructed Rridge Regression model with optimal lamda in the testing set to make statistical inference. This method is used when we believe all the features are important since none of varaibles / features are dropped when constructing a ridge regression. \n",
    "\n",
    "2) Variable Selection where we believe some of variables are more statistically important than others. When choosing variables, forward selection and backward elimination are two ways to decide if variables / features should be added / dropped while AIC, AICC, or BIC are used as a benchmark. \n",
    "\n",
    "Sadly, none of methods mentioned above were covered in class. The method that Professor used to address overfitting was simply to loop through all variables/features and only keep features that are statistically significant at a = 5% by examing if 0 is inside their 95% confidence interval. The rule of thumb is if 0 is inside its 95% confidence interval, then we fail to reject the null hypothesis and conclude that that variable / feature is not statistically significant, and thus that particular variable should be dropped. In this assignment, I did what professor did in the class to address overfitting, although statistically speaking such method is not valid as practitioner should never drop a bunch of variables all at once as a subset of variables may become statistically significant when others are dropped. Instead, a step to step dropping variable/adding variable approach is recommended as I have explained in 2). \n",
    "\n",
    "\n",
    "b) interpretation of the linear regression model results\n",
    "In order to address overfitting, 235 columns are dropped in an attempt to solve multicollinearity problem. However, the result is not as desirable as we thought as multicollinearity is still present. R square also drops as we took out so many columns. However, a model with a bigger R-square doesn't mean it is a better model grounded on the fact that R-square only increases if more variables/features are added to the model ignoring the fact that more complex a model becomes, a higher chance of overfitting. The coeffcient of Beta1 is 0.3377, indicating a unit increase of feature 1 will result in increasing y(number of comments) by 0.3377. Since P>|t| is smaller than 0.05, it is also statistically significant. Similar interpretations prevail to other features' coeffcients and P values. \n",
    "\n",
    "\n",
    "c) compare the results for the two test datasets and mention any interesting observations\n",
    "\n",
    "I have compared the results graphically above, a more numerical apporach could be using MSE(Mean Square Error)\n",
    "\n",
    "MSE of Test blogData_test-2012.02 is: 756.050115665 \n",
    "\n",
    "MSE of Test blogData_test-2012.03 is: 820.708588528\n",
    "\n",
    "It seems that MSE of Test blogData_test-2012.02 is smaller than MSE of Test blogData_test-2012.03, indicating my regression model had a better performance in Test blogData_test-2012.02 than that of Test blogData_test-2012.03. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Next, we will fit a logistic regression model that decides if a blog post is popular or not. In order to do so, note that you need to define a binary attribute on which you can fit the logistic regression model. As before, use the model to analyze the data.\n",
    "\n",
    "1. Report the results of your linear regression model (error, factors, e.t.c.) for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) how did you define the binary attribute, b) interpretation of the logistic regression model results, c) compare the results for the two test datasets and mention any interesting observations ** (5 points) **\n",
    "\n",
    "Total: ** (15 points) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "I define it is popular if the number of comments is greater than 100\n",
      "--------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/CJ/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/CJ/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktSxHCRJHctBktSxHCRJnf8POu/vhLQHqLkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a6502e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define popularity threshold\n",
    "df_train['281'].describe()\n",
    "df_train['281'].hist()\n",
    "\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "print(\"I define it is popular if the number of comments is greater than 100\")\n",
    "print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "df_train[\"popularity\"] = 0\n",
    "for i in range(len(df_train[\"popularity\"])):\n",
    "    if df_train[\"281\"][i] >= 100:\n",
    "        df_train[\"popularity\"][i] = 1\n",
    "\n",
    "df_02[\"popularity\"] = 0\n",
    "for i in range(len(df_02[\"popularity\"])):\n",
    "    if df_02[\"281\"][i] >= 100:\n",
    "        df_02[\"popularity\"][i] = 1\n",
    "\n",
    "df_03[\"popularity\"] = 0\n",
    "for i in range(len(df_03[\"popularity\"])):\n",
    "    if df_03[\"281\"][i] >= 100:\n",
    "        df_03[\"popularity\"][i] = 1\n",
    "\n",
    "\n",
    "y_train = df_train[\"popularity\"] \n",
    "X_train = Xsignif\n",
    "\n",
    "y_test_02 = df_02[\"popularity\"]\n",
    "X_test_02 = df_02_testing_set_X\n",
    "\n",
    "y_test_03 = df_03[\"popularity\"]\n",
    "X_test_03 = df_03_testing_set_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing result for blogData_test-2012.02 is Precision: 0.800, Recall: 0.333, F1 Score: 0.471\n",
      "\n",
      "Testing result for blogData_test-2013.03 is Precision: 0.364, Recall: 0.163, F1 Score: 0.225\n"
     ]
    }
   ],
   "source": [
    "def evaluate(y_train, X_train, y_test, X_test, threshold):\n",
    "    # learn model on training data\n",
    "    model = LogisticRegression()\n",
    "    result = model.fit(X_train,y_train)\n",
    "    # make probability predictions on test data\n",
    "    y_pred = result.predict(X_test)\n",
    "    # threshold probabilities to create classifications\n",
    "    y_pred = y_pred > threshold\n",
    "    # report metrics\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    return precision, recall\n",
    "\n",
    "precision_02, recall_02 = evaluate(y_train, X_train, y_test_02, X_test_02, 0.5)\n",
    "precision_03, recall_03 = evaluate(y_train, X_train, y_test_03, X_test_03, 0.5)\n",
    "\n",
    "def F1_Score(pre,rec):\n",
    "    return 2 * pre * rec / (pre + rec)\n",
    "\n",
    "F1_Score_02 = F1_Score(precision_02,recall_02)\n",
    "F1_Score_03 = F1_Score(precision_03,recall_03)\n",
    "\n",
    "print('\\nTesting result for blogData_test-2012.02 is Precision: {:0.3f}, Recall: {:0.3f}, F1 Score: {:0.3f}'.format(precision_02,recall_02,F1_Score_02))\n",
    "print('\\nTesting result for blogData_test-2013.03 is Precision: {:0.3f}, Recall: {:0.3f}, F1 Score: {:0.3f}'.format(precision_03,recall_03,F1_Score_03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) how did you define the binary attribute\n",
    "\n",
    "I define the popularity meansure through observing histogram. Since most of y value are smaller than 100, I define it is popular if y(the number of comments) is greater than 100\n",
    "\n",
    "b) interpretation of the logistic regression model results\n",
    "Different from a liner regression whose y value is continuous, Logistic regression is concerned with estimating a probability. However, now y(the number of comments) is now a categorical variable, which we will code as 0/1. If y > 100, y = 1; else y = 0. The threshold I define in this problem is 0.5. Basically if p( y = 1 | x) >= 0.5, y = 1, prediced as popular, otherwise p( y = 1 | x) < 0.5, y = 0, prediced as not popular. \n",
    "\n",
    "\n",
    "c) compare the results for the two test datasets and mention any interesting observations\n",
    "\n",
    "\n",
    "\n",
    "$$ Precision = \\frac{True Positives}{True Positives + False Positives} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ Recall = \\frac{True Positives}{True Positives + False Negatives} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$ F1 Score = \\frac{2 Precision · Recall}{Precision + Recall} $$\n",
    "\n",
    "\n",
    "Testing result for blogData_test-2012.02 is Precision: 0.800, Recall: 0.333, F1 Score: 0.471\n",
    "\n",
    "Testing result for blogData_test-2013.03 is Precision: 0.364, Recall: 0.163, F1 Score: 0.225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Now, we are going to use the K-Nearest Neighbors Classifier to decide if a blog post is popular or not. Please use the same popularity definition as in task 2, for this task. KNN is an instant-based classification which simply stores instances of the training data. Then, classification is computed from a majority vote of the nearest neighbors of each point.\n",
    "\n",
    "1. Report the accuracy of your prediction for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) how did you define the nearest neighbors, b) interpretation of the KNN results, c) compare the results for the two test datasets and mention any interesting observations (5 points)\n",
    "\n",
    "Total: ** (15 points) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data blogData_test-2012.02 is: 0.9877213021130783\n",
      "Accuracy on test data blogData_test-2012.03 is: 0.9890776699029126\n"
     ]
    }
   ],
   "source": [
    "k = 5 \n",
    "knn = KNeighborsClassifier(n_neighbors=k, p = 2, metric = 'minkowski')\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred_test_02 = knn.predict(X_test_02)\n",
    "y_pred_test_03 = knn.predict(X_test_03)\n",
    "print('Accuracy on test data blogData_test-2012.02 is: {}'.format(knn.score(X_test_02, y_test_02)))\n",
    "print('Accuracy on test data blogData_test-2012.03 is: {}'.format(knn.score(X_test_03, y_test_03)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) how did you define the nearest neighbors\n",
    "\n",
    "To classify an unknown record: \n",
    "\n",
    "--->1)Compute distance to other training records \n",
    "\n",
    "--->2)Identify k nearest neighbors  \n",
    "\n",
    "--->3) Use class labels of nearest neighbors to determine the class label of unknown record (e.g., by taking majority vote)\n",
    "\n",
    "b) interpretation of the KNN results\n",
    "\n",
    "In this problem, we strictly set up k = 5 so that there is no toss-up situation. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. Following the standard three-step approach above, we generate a knn model from the tranning set and use it to predict outcome of testing set with accuracy as performance metrics. \n",
    "\n",
    "c) compare the results for the two test datasets and mention any interesting observations \n",
    "\n",
    "$$Accuracy = \\frac{True Positives + True Negatives}{True Positives + True Negatives + False Positives + False Negatives} $$\n",
    "\n",
    "Accuracy on test data blogData_test-2012.02 is: 0.9877213021130783\n",
    "\n",
    "Accuracy on test data blogData_test-2012.03 is: 0.9890776699029126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Finally, we are going to use the Decision Trees Classifier to decide if a blog post is popular or not. Please use the same popularity definition as in tasks 2,3, for this task. In order to construct a Decision Tree you will need to discretize some of the attributes.\n",
    "\n",
    "1. Report the accuracy of your prediction for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) which attributes did you discretize and how, b) interpretation of the Decision Tree Classifier results, c) compare the results for the two test datasets and mention any interesting observations (5 points)\n",
    "\n",
    "Total: ** (15 points) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy on test data blogData_test-2012.02 is:  0.985436893204\n",
      "DT accuracy on test data blogData_test-2012.03 is:  0.984466019417\n"
     ]
    }
   ],
   "source": [
    "dtc = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, \n",
    "                                  min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                  max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                                  class_weight=None, presort=False)\n",
    "dtc.fit(X_train,y_train)\n",
    "y_pred_test_tree_02 = dtc.predict(X_test_02)\n",
    "y_pred_test_tree_03 = dtc.predict(X_test_03)\n",
    "print('DT accuracy on test data blogData_test-2012.02 is: ', dtc.score(X_test_02, y_test_02))\n",
    "print('DT accuracy on test data blogData_test-2012.03 is: ', dtc.score(X_test_03, y_test_03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) which attributes did you discretize and how\n",
    "\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split. If The max_features=None, max_features=n_features to consider when looking for the best split. A node will be split if this split induces a decrease of the impurity greater than or equal to 0.\n",
    "\n",
    "b) interpretation of the Decision Tree Classifier results\n",
    "\n",
    "Following the standard splitting procedures described above, we generate a decisiontree model from the tranning set and use it to predict outcome of testing set with accuracy as performance metrics. When we apply model to test data, we start from the root of the tree and just walk down the tree following spliting criteria until reaching a classfication.\n",
    "\n",
    "c) compare the results for the two test datasets and mention any interesting observations\n",
    "\n",
    "$$Accuracy = \\frac{True Positives + True Negatives}{True Positives + True Negatives + False Positives + False Negatives} $$\n",
    "\n",
    "DT accuracy on test data blogData_test-2012.02 is:  0.985436893204\n",
    "\n",
    "DT accuracy on test data blogData_test-2012.03 is:  0.984466019417"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
